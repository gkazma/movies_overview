{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genres</th>\n",
       "      <th>overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Comedy, Drama, Romance]</td>\n",
       "      <td>Beautiful chanteuse 'Bijou' (Marlene Dietrich)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Action, Crime, Drama, Thriller]</td>\n",
       "      <td>In a post-apocalyptic world ravaged by feuding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Drama, Romance, TV Movie, Western]</td>\n",
       "      <td>Marty is a 19 year old pioneer woman, recently...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Action, Comedy]</td>\n",
       "      <td>A couple of fumbling best friends run a privat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Comedy, Romance, TV Movie]</td>\n",
       "      <td>One woman's unexpected race to the altar teach...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                genres  \\\n",
       "0             [Comedy, Drama, Romance]   \n",
       "1     [Action, Crime, Drama, Thriller]   \n",
       "2  [Drama, Romance, TV Movie, Western]   \n",
       "3                     [Action, Comedy]   \n",
       "4          [Comedy, Romance, TV Movie]   \n",
       "\n",
       "                                            overview  \n",
       "0  Beautiful chanteuse 'Bijou' (Marlene Dietrich)...  \n",
       "1  In a post-apocalyptic world ravaged by feuding...  \n",
       "2  Marty is a 19 year old pioneer woman, recently...  \n",
       "3  A couple of fumbling best friends run a privat...  \n",
       "4  One woman's unexpected race to the altar teach...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are multiple genres per movie\n",
    "train_df = pd.read_csv(\"dataset/train_data.csv\",usecols=['genres', 'overview'], converters={\"genres\":literal_eval})\n",
    "test_df = pd.read_csv(\"dataset/test_data.csv\",usecols=['genres', 'overview'], converters={\"genres\":literal_eval})\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial train and test split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_split = 0.1\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=test_split,\n",
    "    stratify=train_df[\"genres\"].values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 20:28:53.858774: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-17 20:28:54.434420: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-06-17 20:28:55.000240: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-17 20:28:55.079026: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-17 20:28:55.079359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-17 20:28:55.080240: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-17 20:28:55.080695: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-17 20:28:55.080968: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-17 20:28:55.531307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-17 20:28:55.531986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-17 20:28:55.532318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-17 20:28:55.532562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6901 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-06-17 20:28:55.630729: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype variant and shape [33420]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "\n",
      "['Drama', 'Comedy', 'Thriller', 'Romance', 'Action', 'Horror', 'Crime', 'Documentary', 'Adventure', 'Science Fiction', 'Family', 'Mystery', 'Fantasy', 'Animation', 'Music', 'Foreign', 'History', 'War', 'Western', 'TV Movie']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "genres = tf.ragged.constant(train_df[\"genres\"].values)\n",
    "lookup = tf.keras.layers.StringLookup(output_mode=\"multi_hot\", num_oov_indices=0)\n",
    "lookup.adapt(genres)\n",
    "vocab = lookup.get_vocabulary()\n",
    "\n",
    "print(\"Vocabulary:\\n\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(dataframe, is_train=True):\n",
    "    labels = tf.ragged.constant(dataframe[\"genres\"].values)\n",
    "    label_binarized = lookup(labels).numpy()\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (dataframe[\"overview\"].values, label_binarized)\n",
    "    )\n",
    "    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "train_dataset = make_dataset(train_df, is_train=True)\n",
    "validation_dataset = make_dataset(val_df, is_train=False)\n",
    "test_dataset = make_dataset(test_df, is_train=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "def make_mlp_model(text_vectorizer):\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            tf.keras.Input(shape=(1,), dtype=tf.string, name='text'),\n",
    "            text_vectorizer,\n",
    "            layers.Dense(64, activation=\"relu\"),\n",
    "            layers.Dense(32, activation=\"relu\"),\n",
    "            layers.Dense(lookup.vocabulary_size(), activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_conv_model(text_vectorizer):\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            tf.keras.Input(shape=(1,), dtype=tf.string, name='text'),\n",
    "            text_vectorizer,\n",
    "            layers.Lambda(lambda x: tf.cast(x, \"float32\")),\n",
    "            layers.Reshape((-1, 1)),\n",
    "            layers.Conv1D(64, 3, activation='sigmoid'),\n",
    "            layers.Conv1D(32, 3, activation='sigmoid'),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(lookup.vocabulary_size(), activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_lstm_model(text_vectorizer):\n",
    "    model = tf.keras.Sequential([\n",
    "        text_vectorizer,\n",
    "        layers.Embedding(\n",
    "            input_dim=len(text_vectorizer.get_vocabulary()),\n",
    "            output_dim=64,\n",
    "            mask_zero=True),\n",
    "        layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(lookup.vocabulary_size(), activation=\"sigmoid\"),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_stacked_lstm_model(text_vectorizer):\n",
    "    model = tf.keras.Sequential([\n",
    "        text_vectorizer,\n",
    "        tf.keras.layers.Embedding(len(text_vectorizer.get_vocabulary()), 64, mask_zero=True),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        layers.Dense(lookup.vocabulary_size(), activation=\"sigmoid\"),\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_result(history, item):\n",
    "    plt.plot(history.history[item], label=item)\n",
    "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(item)\n",
    "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorization with MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 14s 58ms/step - loss: 0.2964 - binary_accuracy: 0.9006 - categorical_accuracy: 0.4824 - accuracy: 0.0000e+00 - auc_1: 0.7936 - f1_score: 0.1736 - precision_1: 0.5522 - recall_1: 0.2497 - val_loss: 0.2233 - val_binary_accuracy: 0.9175 - val_categorical_accuracy: 0.5356 - val_accuracy: 0.0000e+00 - val_auc_1: 0.8838 - val_f1_score: 0.2645 - val_precision_1: 0.6868 - val_recall_1: 0.3732\n",
      "Epoch 2/2\n",
      "235/235 [==============================] - 13s 55ms/step - loss: 0.1428 - binary_accuracy: 0.9461 - categorical_accuracy: 0.6125 - accuracy: 2.3273e-05 - auc_1: 0.9581 - f1_score: 0.3927 - precision_1: 0.8398 - recall_1: 0.5968 - val_loss: 0.2318 - val_binary_accuracy: 0.9187 - val_categorical_accuracy: 0.5111 - val_accuracy: 1.4961e-05 - val_auc_1: 0.8862 - val_f1_score: 0.3013 - val_precision_1: 0.6530 - val_recall_1: 0.4582\n",
      "33/33 [==============================] - 0s 15ms/step - loss: 0.2242 - binary_accuracy: 0.9181 - categorical_accuracy: 0.5337 - accuracy: 0.0000e+00 - auc_1: 0.8822 - f1_score: 0.2619 - precision_1: 0.6889 - recall_1: 0.3769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22423888742923737,\n",
       " 0.9180562496185303,\n",
       " 0.5336887836456299,\n",
       " 0.0,\n",
       " 0.8821662068367004,\n",
       " 0.2619091272354126,\n",
       " 0.6888936758041382,\n",
       " 0.3769357204437256]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer = layers.TextVectorization(output_mode=\"tf_idf\")\n",
    "text_vectorizer.adapt(train_dataset.map(lambda text, label: text))\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "model = make_mlp_model(text_vectorizer)\n",
    "\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\", \n",
    "    optimizer=\"adam\",\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy(), \n",
    "             tf.keras.metrics.CategoricalAccuracy(), \n",
    "             tf.keras.metrics.Accuracy(), \n",
    "             tf.keras.metrics.AUC(), \n",
    "             tf.keras.metrics.F1Score(average='macro'), \n",
    "             tf.keras.metrics.Precision(), \n",
    "             tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=0,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=epochs, callbacks=[early_stopping_monitor], verbose=1\n",
    ")\n",
    "\n",
    "# plot_result(history, \"loss\")\n",
    "# plot_result(history, \"binary_accuracy\")\n",
    "\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "235/235 [==============================] - 4s 16ms/step - loss: 0.2934 - binary_accuracy: 0.8927 - categorical_accuracy: 0.4688 - accuracy: 0.0000e+00 - auc_3: 0.7530 - f1_score: 0.0331 - precision_3: 0.4134 - recall_3: 0.0676 - val_loss: 0.2854 - val_binary_accuracy: 0.8960 - val_categorical_accuracy: 0.4734 - val_accuracy: 0.0000e+00 - val_auc_3: 0.7654 - val_f1_score: 0.0321 - val_precision_3: 0.4885 - val_recall_3: 0.1017\n",
      "Epoch 2/2\n",
      "235/235 [==============================] - 3s 14ms/step - loss: 0.2866 - binary_accuracy: 0.8952 - categorical_accuracy: 0.4737 - accuracy: 0.0000e+00 - auc_3: 0.7643 - f1_score: 0.0322 - precision_3: 0.4837 - recall_3: 0.0655 - val_loss: 0.2851 - val_binary_accuracy: 0.8965 - val_categorical_accuracy: 0.4734 - val_accuracy: 0.0000e+00 - val_auc_3: 0.7635 - val_f1_score: 0.0321 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.2844 - binary_accuracy: 0.8967 - categorical_accuracy: 0.4716 - accuracy: 0.0000e+00 - auc_3: 0.7651 - f1_score: 0.0320 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28439009189605713,\n",
       " 0.896703839302063,\n",
       " 0.47164323925971985,\n",
       " 0.0,\n",
       " 0.7651023864746094,\n",
       " 0.032048750668764114,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seqlen = 150\n",
    "text_vectorizer = layers.TextVectorization(output_sequence_length=max_seqlen, output_mode=\"int\")\n",
    "text_vectorizer.adapt(train_dataset.map(lambda text, label: text))\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "model = make_conv_model(text_vectorizer)\n",
    "\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\", \n",
    "    optimizer=\"adam\",\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy(), \n",
    "             tf.keras.metrics.CategoricalAccuracy(), \n",
    "             tf.keras.metrics.Accuracy(), \n",
    "             tf.keras.metrics.AUC(), \n",
    "             tf.keras.metrics.F1Score(average='macro'), \n",
    "             tf.keras.metrics.Precision(), \n",
    "             tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=0,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=epochs, callbacks=[early_stopping_monitor], verbose=1\n",
    ")\n",
    "\n",
    "# plot_result(history, \"loss\")\n",
    "# plot_result(history, \"binary_accuracy\")\n",
    "\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "text_vectorizer = layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "text_vectorizer.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "262/262 [==============================] - 28s 88ms/step - loss: 0.3138 - binary_accuracy: 0.8885 - categorical_accuracy: 0.4376 - accuracy: 0.0000e+00 - auc: 0.7306 - f1_score: 0.0370 - precision: 0.3192 - recall: 0.0609 - val_loss: 0.2833 - val_binary_accuracy: 0.8969 - val_categorical_accuracy: 0.4704 - val_accuracy: 0.0000e+00 - val_auc: 0.7685 - val_f1_score: 0.0320 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/2\n",
      "262/262 [==============================] - 23s 88ms/step - loss: 0.2754 - binary_accuracy: 0.8978 - categorical_accuracy: 0.4683 - accuracy: 0.0000e+00 - auc: 0.7941 - f1_score: 0.0470 - precision: 0.5527 - recall: 0.1048 - val_loss: 0.2649 - val_binary_accuracy: 0.8996 - val_categorical_accuracy: 0.4596 - val_accuracy: 0.0000e+00 - val_auc: 0.8184 - val_f1_score: 0.0540 - val_precision: 0.5815 - val_recall: 0.0927\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.2664 - binary_accuracy: 0.8996 - categorical_accuracy: 0.4578 - accuracy: 0.0000e+00 - auc: 0.8153 - f1_score: 0.0534 - precision: 0.5903 - recall: 0.0931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.26637598872184753,\n",
       " 0.8996485471725464,\n",
       " 0.4578284025192261,\n",
       " 0.0,\n",
       " 0.8152556419372559,\n",
       " 0.053361546248197556,\n",
       " 0.5903345942497253,\n",
       " 0.09314875304698944]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "model = make_lstm_model(text_vectorizer)\n",
    "\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\", \n",
    "    optimizer=\"adam\",\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy(), \n",
    "             tf.keras.metrics.CategoricalAccuracy(), \n",
    "             tf.keras.metrics.Accuracy(), \n",
    "             tf.keras.metrics.AUC(), \n",
    "             tf.keras.metrics.F1Score(average='macro'), \n",
    "             tf.keras.metrics.Precision(), \n",
    "             tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=0,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=epochs, callbacks=[early_stopping_monitor], verbose=1\n",
    ")\n",
    "\n",
    "# plot_result(history, \"loss\")\n",
    "# plot_result(history, \"binary_accuracy\")\n",
    "\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "262/262 [==============================] - 54s 173ms/step - loss: 0.3394 - binary_accuracy: 0.8781 - categorical_accuracy: 0.3626 - accuracy: 0.0000e+00 - auc_1: 0.6851 - f1_score: 0.0506 - precision_1: 0.2967 - recall_1: 0.1235 - val_loss: 0.2833 - val_binary_accuracy: 0.8969 - val_categorical_accuracy: 0.4704 - val_accuracy: 0.0000e+00 - val_auc_1: 0.7677 - val_f1_score: 0.0320 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 2/2\n",
      "262/262 [==============================] - 45s 170ms/step - loss: 0.2942 - binary_accuracy: 0.8956 - categorical_accuracy: 0.4607 - accuracy: 0.0000e+00 - auc_1: 0.7490 - f1_score: 0.0384 - precision_1: 0.4963 - recall_1: 0.0954 - val_loss: 0.2741 - val_binary_accuracy: 0.9006 - val_categorical_accuracy: 0.4707 - val_accuracy: 0.0000e+00 - val_auc_1: 0.7957 - val_f1_score: 0.0322 - val_precision_1: 0.5988 - val_recall_1: 0.1072\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.2753 - binary_accuracy: 0.9007 - categorical_accuracy: 0.4719 - accuracy: 0.0000e+00 - auc_1: 0.7935 - f1_score: 0.0324 - precision_1: 0.6127 - recall_1: 0.1062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27534621953964233,\n",
       " 0.9007391929626465,\n",
       " 0.4718855917453766,\n",
       " 0.0,\n",
       " 0.7935128808021545,\n",
       " 0.032402507960796356,\n",
       " 0.612728476524353,\n",
       " 0.10617081075906754]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "model = make_stacked_lstm_model(text_vectorizer)\n",
    "\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\", \n",
    "    optimizer=\"adam\",\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy(), \n",
    "             tf.keras.metrics.CategoricalAccuracy(), \n",
    "             tf.keras.metrics.Accuracy(), \n",
    "             tf.keras.metrics.AUC(), \n",
    "             tf.keras.metrics.F1Score(average='macro'), \n",
    "             tf.keras.metrics.Precision(), \n",
    "             tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=0,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=epochs, callbacks=[early_stopping_monitor], verbose=1\n",
    ")\n",
    "\n",
    "# plot_result(history, \"loss\")\n",
    "# plot_result(history, \"binary_accuracy\")\n",
    "\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghaith/miniconda3/envs/movies_overview/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "from tfhub_maps import *\n",
    "\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movies",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
